{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1picaD1cpAUk"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# HEPSIBURADA ADDRESS MATCHING HYBRID PIPELINE - FULL TRAINING FROM SCRATCH\n",
        "# Architecture: BGE-M3 + BM25 + BERT Reranker\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import time\n",
        "import pickle\n",
        "import pathlib\n",
        "import zipfile\n",
        "import glob\n",
        "import gc\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
        "from contextlib import contextmanager\n",
        "from typing import Dict, List, Tuple\n",
        "import bm25s\n",
        "\n",
        "# CUDA Memory Optimization\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.BASE_PATH = \"/content/\"\n",
        "        self.DRIVE_SAVE_PATH = \"/content/drive/MyDrive/Hepsiburada_AddressMatching_Scratch/\"\n",
        "        self.BGE_MODEL_NAME = 'BAAI/bge-m3'\n",
        "        self.BERT_MODEL_NAME = \"dbmdz/bert-base-turkish-cased\"\n",
        "\n",
        "        # New model directories for full training\n",
        "        self.BERT_MODEL_DIR_BEST = \"bert_full_train_best\"\n",
        "        self.BERT_MODEL_DIR_LATEST = \"bert_full_train_latest\"\n",
        "\n",
        "        # Retrieval Hyperparameters\n",
        "        self.K_NEIGHBORS_BGE = 30\n",
        "        self.K_NEIGHBORS_BM25 = 30\n",
        "        self.K_NEIGHBORS_HARD_MINING = 20\n",
        "\n",
        "        # Batch Sizes\n",
        "        self.BATCH_SIZE_EMBEDDING = 256\n",
        "        self.BATCH_SIZE_BERT_TRAIN = 64\n",
        "        self.BATCH_SIZE_GPU_SEARCH = 256\n",
        "\n",
        "        # Training Settings\n",
        "        self.NUM_EPOCHS = 2\n",
        "        self.VALIDATION_SPLIT = 0.15\n",
        "        self.RANDOM_STATE = 42\n",
        "\n",
        "        # Filenames\n",
        "        self.PROCESSED_TRAIN_FILE = \"train_processed.csv\"\n",
        "        self.PROCESSED_TEST_FILE = \"test_processed.csv\"\n",
        "        self.EMBEDDINGS_FILE = \"bge_m3_embeddings.npy\"\n",
        "        self.BM25_INDEX_DIR = \"bm25s_index\"\n",
        "        self.BERT_CANDIDATES_FILE = f\"bert_candidates_hybrid.pkl\"\n",
        "        self.BERT_TRAIN_FILE = \"bert_train_dataset_full.csv\"\n",
        "        self.SUBMISSION_FILE = \"submission_scratch.csv\"\n",
        "\n",
        "    def get_path(self, filename: str) -> str:\n",
        "        return os.path.join(self.DRIVE_SAVE_PATH, filename)\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# =============================================================================\n",
        "# TEXT PREPROCESSING\n",
        "# =============================================================================\n",
        "def turkish_to_lower(text: str) -> str:\n",
        "    return str(text).replace('İ', 'i').replace('I', 'ı').lower()\n",
        "\n",
        "ABBREVIATIONS = {\n",
        "    r\"\\bmh\\b|\\bmah\\b|\\bmahalle\\b\": \"mahallesi\", r\"\\bcd\\b|\\bcad\\b|\\bcaddesi\\b\": \"caddesi\",\n",
        "    r\"\\bsk\\b|\\bsok\\b|\\bsokağı\\b\": \"sokak\", r\"\\bblv\\b|\\bbulvarı\\b\": \"bulvar\",\n",
        "    r\"\\bapt\\b|\\baprt\\b|\\bapartmanı\\b\": \"apartman\", r\"\\bst\\b|\\bsitesi\\b\": \"site\",\n",
        "    r\"\\bno\\b|\\baptno\\b\": \"numara\", r\"\\bk\\b|\\bkatı\\b|\\bkat\\b\": \"kat\",\n",
        "}\n",
        "\n",
        "def hybrid_preprocess(text: str) -> str:\n",
        "    if pd.isnull(text): return \"\"\n",
        "    text = turkish_to_lower(text)\n",
        "    text = re.sub(r\"(\\d+)([a-zA-Zçğıöşü]+)\", r\"\\1 \\2\", text)\n",
        "    text = re.sub(r\"([a-zA-Zçğıöşü]+)(\\d+)\", r\"\\1 \\2\", text)\n",
        "    for pattern, repl in ABBREVIATIONS.items():\n",
        "        text = re.sub(pattern, f\" {repl} \", text)\n",
        "    text = re.sub(r\"[^a-zçğıöşü0-9\\-\\/\\s]\", \" \", text)\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# =============================================================================\n",
        "# GPU OPTIMIZED SEARCH ENGINE\n",
        "# =============================================================================\n",
        "class VectorSearch:\n",
        "    def __init__(self, embeddings: np.ndarray):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.embeddings_norm = torch.nn.functional.normalize(\n",
        "            torch.from_numpy(embeddings).to(self.device, dtype=torch.float32)\n",
        "        )\n",
        "\n",
        "    def search(self, queries: np.ndarray, k: int, batch_size: int) -> np.ndarray:\n",
        "        all_indices = []\n",
        "        for i in range(0, len(queries), batch_size):\n",
        "            batch = torch.from_numpy(queries[i:i + batch_size]).to(self.device, dtype=torch.float32)\n",
        "            batch_norm = torch.nn.functional.normalize(batch)\n",
        "            sims = torch.mm(batch_norm, self.embeddings_norm.T)\n",
        "            all_indices.append(torch.topk(sims, k)[1].cpu().numpy())\n",
        "        return np.vstack(all_indices)\n",
        "\n",
        "# =============================================================================\n",
        "# PHASE 1: DATA PREP & RETRIEVAL\n",
        "# =============================================================================\n",
        "class Phase1Engine:\n",
        "    def __init__(self, cfg: Config):\n",
        "        self.cfg = cfg\n",
        "\n",
        "    def run(self) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        os.makedirs(self.cfg.DRIVE_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "        # 1. Load Data\n",
        "        train_path = self.cfg.get_path(self.cfg.PROCESSED_TRAIN_FILE)\n",
        "        test_path = self.cfg.get_path(self.cfg.PROCESSED_TEST_FILE)\n",
        "\n",
        "        if os.path.exists(train_path):\n",
        "            print(\"--- Loading Preprocessed Data ---\")\n",
        "            df_train = pd.read_csv(train_path).fillna('')\n",
        "            df_test = pd.read_csv(test_path).fillna('')\n",
        "        else:\n",
        "            print(\"--- Preprocessing Raw Data ---\")\n",
        "            df_train = pd.read_csv(os.path.join(self.cfg.BASE_PATH, \"train.csv\"))\n",
        "            df_test = pd.read_csv(os.path.join(self.cfg.BASE_PATH, \"test.csv\"))\n",
        "            df_train['clean_address'] = df_train['address'].apply(hybrid_preprocess)\n",
        "            df_test['clean_address'] = df_test['address'].apply(hybrid_preprocess)\n",
        "            df_train.to_csv(train_path, index=False)\n",
        "            df_test.to_csv(test_path, index=False)\n",
        "\n",
        "        # 2. BGE-M3 Embeddings (The Foundation)\n",
        "        emb_path = self.cfg.get_path(self.cfg.EMBEDDINGS_FILE)\n",
        "        from FlagEmbedding import BGEM3FlagModel\n",
        "        bge_model = BGEM3FlagModel(self.cfg.BGE_MODEL_NAME, use_fp16=True)\n",
        "\n",
        "        if os.path.exists(emb_path):\n",
        "            train_embs = np.load(emb_path)\n",
        "        else:\n",
        "            print(\"--- Generating Train Embeddings ---\")\n",
        "            train_embs = []\n",
        "            for i in tqdm(range(0, len(df_train), self.cfg.BATCH_SIZE_EMBEDDING)):\n",
        "                batch_txt = df_train['clean_address'].iloc[i:i+self.cfg.BATCH_SIZE_EMBEDDING].tolist()\n",
        "                train_embs.append(bge_model.encode(batch_txt)['dense_vecs'])\n",
        "            train_embs = np.vstack(train_embs).astype('float32')\n",
        "            np.save(emb_path, train_embs)\n",
        "\n",
        "        # 3. Hybrid Candidate Search for Test Set\n",
        "        cand_path = self.cfg.get_path(self.cfg.BERT_CANDIDATES_FILE)\n",
        "        if os.path.exists(cand_path):\n",
        "            with open(cand_path, 'rb') as f: candidates = pickle.load(f)\n",
        "        else:\n",
        "            print(\"--- Searching Candidates (Hybrid: BGE + BM25) ---\")\n",
        "            # BGE Search\n",
        "            test_embs = bge_model.encode(df_test['clean_address'].tolist())['dense_vecs']\n",
        "            v_search = VectorSearch(train_embs)\n",
        "            bge_idx = v_search.search(test_embs, k=self.cfg.K_NEIGHBORS_BGE, batch_size=self.cfg.BATCH_SIZE_GPU_SEARCH)\n",
        "\n",
        "            # BM25 Search\n",
        "            bm25 = bm25s.BM25()\n",
        "            bm25.index([d.split() for d in df_train['clean_address']])\n",
        "            bm25_idx, _ = bm25.retrieve([q.split() for q in df_test['clean_address']], k=self.cfg.K_NEIGHBORS_BM25)\n",
        "\n",
        "            candidates = {row['id']: list(set(bge_idx[i]).union(set(bm25_idx[i])))\n",
        "                          for i, row in df_test.iterrows()}\n",
        "            with open(cand_path, 'wb') as f: pickle.dump(candidates, f)\n",
        "\n",
        "        # 4. Create Hard Negatives for BERT Training\n",
        "        hn_path = self.cfg.get_path(self.cfg.BERT_TRAIN_FILE)\n",
        "        if not os.path.exists(hn_path):\n",
        "            print(\"--- Generating Hard Negative Dataset for BERT ---\")\n",
        "            v_search = VectorSearch(train_embs)\n",
        "            indices = v_search.search(train_embs, k=self.cfg.K_NEIGHBORS_HARD_MINING, batch_size=self.cfg.BATCH_SIZE_GPU_SEARCH)\n",
        "            labels, addrs = df_train['label'].values, df_train['clean_address'].values\n",
        "\n",
        "            pairs = []\n",
        "            for i in tqdm(range(len(df_train))):\n",
        "                for n_idx in indices[i]:\n",
        "                    if i != n_idx:\n",
        "                        pairs.append({'sentence1': addrs[i], 'sentence2': addrs[n_idx], 'label': 1 if labels[i] == labels[n_idx] else 0})\n",
        "\n",
        "            hn_df = pd.DataFrame(pairs)\n",
        "            pos, neg = hn_df[hn_df.label == 1], hn_df[hn_df.label == 0]\n",
        "            balanced_df = pd.concat([pos, neg.sample(len(pos))]).sample(frac=1)\n",
        "            balanced_df.to_csv(hn_path, index=False)\n",
        "\n",
        "        del bge_model; gc.collect(); torch.cuda.empty_cache()\n",
        "        return df_train, df_test, candidates\n",
        "\n",
        "# =============================================================================\n",
        "# PHASE 2: BERT RERANKER TRAINING & SUBMISSION\n",
        "# =============================================================================\n",
        "class Phase2Engine:\n",
        "    def __init__(self, cfg: Config):\n",
        "        self.cfg = cfg\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(cfg.BERT_MODEL_NAME)\n",
        "\n",
        "    def train_reranker(self):\n",
        "        print(\"--- Starting BERT Reranker Training from Scratch ---\")\n",
        "        df = pd.read_csv(self.cfg.get_path(self.cfg.BERT_TRAIN_FILE)).dropna()\n",
        "        train_df, val_df = train_test_split(df, test_size=self.cfg.VALIDATION_SPLIT, stratify=df['label'], random_state=self.cfg.RANDOM_STATE)\n",
        "\n",
        "        def tokenize_fn(ex):\n",
        "            return self.tokenizer(ex['sentence1'], ex['sentence2'], truncation=True, max_length=128, padding='max_length')\n",
        "\n",
        "        train_ds = Dataset.from_pandas(train_df).map(tokenize_fn, batched=True)\n",
        "        val_ds = Dataset.from_pandas(val_df).map(tokenize_fn, batched=True)\n",
        "\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(self.cfg.BERT_MODEL_NAME, num_labels=2)\n",
        "\n",
        "        args = TrainingArguments(\n",
        "            output_dir='./results',\n",
        "            num_train_epochs=self.cfg.NUM_EPOCHS,\n",
        "            learning_rate=2e-5,\n",
        "            per_device_train_batch_size=self.cfg.BATCH_SIZE_BERT_TRAIN,\n",
        "            gradient_accumulation_steps=8,\n",
        "            fp16=True,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=2000,\n",
        "            save_strategy=\"steps\",\n",
        "            save_steps=2000,\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"f1\",\n",
        "            report_to=\"none\"\n",
        "        )\n",
        "\n",
        "        def compute_metrics(p):\n",
        "            preds = np.argmax(p.predictions, axis=1)\n",
        "            return {'f1': f1_score(p.label_ids, preds), 'acc': accuracy_score(p.label_ids, preds)}\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds,\n",
        "            tokenizer=self.tokenizer, compute_metrics=compute_metrics\n",
        "        )\n",
        "        trainer.train()\n",
        "\n",
        "        # Save results\n",
        "        trainer.save_model(self.cfg.get_path(self.cfg.BERT_MODEL_DIR_BEST))\n",
        "        model.save_pretrained(self.cfg.get_path(self.cfg.BERT_MODEL_DIR_LATEST))\n",
        "        self.tokenizer.save_pretrained(self.cfg.get_path(self.cfg.BERT_MODEL_DIR_LATEST))\n",
        "        return model\n",
        "\n",
        "    def create_submission(self, model, df_train, df_test, candidates):\n",
        "        print(\"--- Creating Final Submission ---\")\n",
        "        model.eval().cuda()\n",
        "        train_labels, train_addrs = df_train['label'].values, df_train['clean_address'].values\n",
        "        test_map = dict(zip(df_test.id, df_test.clean_address))\n",
        "\n",
        "        results = []\n",
        "        for tid, c_indices in tqdm(candidates.items(), desc=\"Reranking\"):\n",
        "            if not c_indices: continue\n",
        "            t_addr = test_map[tid]\n",
        "            c_addrs = [train_addrs[i] for i in c_indices]\n",
        "\n",
        "            inputs = self.tokenizer([t_addr]*len(c_addrs), c_addrs, return_tensors='pt', padding=True, truncation=True, max_length=128).to('cuda')\n",
        "            with torch.no_grad():\n",
        "                scores = model(**inputs).logits.softmax(dim=-1)[:, 1]\n",
        "                best_match_idx = c_indices[scores.argmax().item()]\n",
        "                results.append({'id': tid, 'label': train_labels[best_match_idx]})\n",
        "\n",
        "        final_df = pd.merge(df_test[['id']], pd.DataFrame(results), on='id', how='left')\n",
        "        final_df['label'] = final_df['label'].fillna(df_train['label'].mode()[0]).astype(int)\n",
        "        final_df.to_csv(self.cfg.get_path(self.cfg.SUBMISSION_FILE), index=False)\n",
        "        print(f\"Submission saved to: {self.cfg.SUBMISSION_FILE}\")\n",
        "\n",
        "# =============================================================================\n",
        "# RUN PIPELINE\n",
        "# =============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "\n",
        "    # 1. Retrieval Phase\n",
        "    p1 = Phase1Engine(config)\n",
        "    d_train, d_test, cands = p1.run()\n",
        "\n",
        "    # 2. Training Phase\n",
        "    p2 = Phase2Engine(config)\n",
        "    best_model = p2.train_reranker()\n",
        "\n",
        "    # 3. Final Ranking\n",
        "    p2.create_submission(best_model, d_train, d_test, cands)\n",
        "\n",
        "    print(f\"✅ Full process completed in {(time.time() - start_time)/60:.2f} minutes.\")"
      ]
    }
  ]
}